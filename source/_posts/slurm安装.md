title: slurm安装
date: 2015-09-10 17:17:44
tags: [slurm,技术,ubuntu]
---
### 说明  
>Slurm is an open-source workload manager designed for Linux clusters of all sizes. It provides three key functions. First it allocates exclusive and/or non-exclusive access to resources (computer nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (typically a parallel job) on a set of allocated nodes. Finally, it arbitrates contention for resources by managing a queue of pending work.  

系统：ubuntu 14.04。slurm版本2.6.5。  
节点：2个独立的ubuntu14.04计算机。
<!--more-->

### 安装
在ubuntu中，最简单的安装方法是使用apt-get，目前自动安装的slurm版本为2.6.5，并不是最新版本。  
{% codeblock lang:sh %}
$ sudo apt-get install slurm-llnl
{% endcodeblock %}  

安装slurm的同时，会自动创建一个蛋疼的slurm用户，但是这个用户没有home文件，所以无法切换到该用户下进行工作。我的建议是删除旧的slurm用户，同时也是为了munge的正确运行做准备。  
{% codeblock lang:sh %}
$ sudo deluser slurm
{% endcodeblock %}  
使用slurm，所有的节点必须在相同的用户下工作，建议新建一个slurm用户，必须指定UID，保证所有节点slurm用户的UID也相同：  
{% codeblock lang:sh %}
$ sudo adduser --uid <ID> <new name>
{% endcodeblock %}

### Munge  
在安装slurm的同时，也会安装[munge](https://github.com/dun/munge/wiki/Installation-Guide)。  
>MUNGE (MUNGE Uid 'N' Gid Emporium) is an authentication service for creating and validating credentials. It is designed to be highly scalable for use in an HPC cluster environment. It allows a process to authenticate the UID and GID of another local or remote process within a group of hosts having common users and groups. These hosts form a security realm that is defined by a shared cryptographic key. Clients within this security realm can create and validate credentials without the use of root privileges, reserved ports, or platform-specific methods.  

1.生成key  
首先生成key，自动生成至`/ete/munge/munge.key`：  
{% codeblock lang:sh %}
$ create-munge-key
{% endcodeblock %}

2.部署  
将key拷贝至所有节点。拷贝`/etc/munge/munge.key`到所有节点的`/etc/munge/`目录下。

3.启动  
启动munge。  
{% codeblock lang:sh %}
$ sudo /etc/init.d/munge start
{% endcodeblock %}  

出现错误：
{% codeblock lang:sh %}
* Starting MUNGE munged                                                 [fail]
munged: Error: Logfile is insecure: group-writable permissions set on "/var/log"
{% endcodeblock %}  
出现上述错误需要修改log文件权限：  
{% codeblock lang:sh %}
$ sudo chmod g-w /var/log
{% endcodeblock %}  

出现错误：
{% codeblock lang:sh %}
* Starting MUNGE munged                                           [fail]
munged: Error: Logfile is insecure: invalid ownership of "/var/log/munge"  
{% endcodeblock %}  
首先尝试使用sudo启动，若不成功，则参考如下。munge安装的所有文件，默认用户为munge:munge，当前用户如果不是munge，启动出现错误：  
需要修改如下文件的用户：
/etc/munge/  
/var/lib/munge/  
/var/log/munge/  
/var/run/munge/  
修改文件用户的命令：  
{% codeblock lang:sh %}
$ sudo chown -R <user>:<group> <dir>  
{% endcodeblock %}
修改/ete/init.d/munge，将USER变量修改为你的用户名。

4.验证  
在slurm用户下验证本机：
{% codeblock lang:sh %}
$ munge -n | unmunge
STATUS:           Success (0)
ENCODE_HOST:      localhost (127.0.0.1)
ENCODE_TIME:      2015-09-14 16:39:58 +0800 (1442219998)
DECODE_TIME:      2015-09-14 16:39:58 +0800 (1442219998)
TTL:              300
CIPHER:           aes128 (4)
MAC:              sha1 (3)
ZIP:              none (0)
UID:              slurm (1023)
GID:              slurm (1023)
LENGTH:           0
{% endcodeblock %}  

验证其它节点：
{% codeblock lang:sh %}
$ munge -n | ssh node01 unmunge
STATUS:           Success (0)
ENCODE_HOST:      localhost (127.0.0.1)
ENCODE_TIME:      2015-09-14 16:40:09 +0800 (1442220009)
DECODE_TIME:      2015-09-14 16:39:49 +0800 (1442219989)
TTL:              300
CIPHER:           aes128 (4)
MAC:              sha1 (3)
ZIP:              none (0)
UID:              slurm (1023)
GID:              slurm (1023)
LENGTH:           0
{% endcodeblock %}  

注意！munge自动认证的是相同的用户，即UID和GID相同，并不是按照用户名称来匹配，所以在创建slurm用户时必须指定相同的UID和GID。  

### slurm配置文件  
在安装文件中有配置工具：  
`slurm-xxx/doc/html/configurator.html.in`  
如果使用apt-get安装，则需要根据自己slurm的版本信息下载相应的tar文件，解压后使用，[slurm所有版本](http://www.schedmd.com/#archives)。  

配置文件样例：
{% codeblock %}
# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine=node00
#ControlAddr=
#BackupController=
#BackupAddr=
#
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/none
CryptoType=crypto/munge
#DisableRootJobs=NO
#EnforcePartLimits=NO
#Epilog=
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=999999
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobCheckpointDir=/var/slurm/checkpoint
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=1
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=5000
#MaxStepCount=40000
#MaxTasksPerNode=128
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/pgid
#Prolog=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/home/slurm/Slurm
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/home/slurm/Slurm/out
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/none
#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=30
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=300
#
#
# SCHEDULING
#DefMemPerCPU=0
FastSchedule=1
#MaxMemPerCPU=0
#SchedulerRootFilter=1
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/linear
#SelectTypeParameters=
#
#
# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/basic
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=
#
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/none
#AccountingStorageUser=
AccountingStoreJobComment=YES
ClusterName=cluster
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=3
#SlurmctldLogFile=
SlurmdDebug=3
#SlurmdLogFile=
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
NodeName=node01 CPUs=1 State=UNKNOWN
NodeName=node00 CPUs=1 State=UNKNOWN
PartitionName=debug Nodes=node00,node01 Default=YES MaxTime=INFINITE State=UP
{% endcodeblock %}  

将该配置文件拷贝至所有节点，放置于/etc/slurm-llnl/slurm.conf中。

### 启动和测试slurm  
到所有节点中启动：
{% codeblock lang:sh %}
$ sudo /ete/init.d/slurm-llnl start
{% endcodeblock %}  

查看节点信息：
{% codeblock lang:sh %}
$ sinfo
{% endcodeblock %}

测试脚本test：
{% codeblock lang:sh %}
#!/bin/bash
#SBATCH -A <account>
#SBATCH -D /home/slurm/Slurm
#SBATCH -I
#SBATCH --time=00:30:00
#SBATCH --error=job.%J.err
#SBATCH --output=job.%J.out

echo "hello world!"
./hello.sh
{% endcodeblock %}

运行测试脚本:
{% codeblock lang:sh %}
$ sbatch test
{% endcodeblock %}

当某一个job因为某些I/O原因阻塞以后，会导致某个节点一直处于comp状态，重启该节点可以使用如下命令：  
{% codeblock lang:sh %}
$ scontrol update NodeName=OptiPlex State=DOWN Reason=hung_completing
$ scontrol update NodeName=OptiPlex State=RESUME
{% endcodeblock %}
